{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of NLP_HW2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JinHAN7/1011_HW2/blob/master/Copy_of_NLP_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ZzoSVhT_wZcB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1. Preparation and Preprocessing \n",
        "- Install pytoch 0.4.1\n",
        "- Cuda version: 9.2\n",
        "- Load data and preprocessing for training"
      ]
    },
    {
      "metadata": {
        "id": "PazlNpZgiE6T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# NVIDIA profiling tool for the available GPU\n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o3xL5WYd92BB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3kYHpLrXnK29",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use PyTorch to check versions, CUDA version and cuDNN\n",
        "\n",
        "import torch\n",
        "\n",
        "print(\"PyTorch version: \")\n",
        "print(torch.__version__)\n",
        "print(\"CUDA Version: \")\n",
        "print(torch.version.cuda)\n",
        "print(\"cuDNN version is: \")\n",
        "print(torch.backends.cudnn.version())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7QQERHbZPdzj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lo4bpleeOVOs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#connect with google drive to load pre-trained embedding \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jMTzuCVRV50w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#load necessary library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle as pkl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FdUdmnw9WYCG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load pre-trained embedding and add <pad> and <unk>\n",
        "PAD_IDX = 0\n",
        "UNK_IDX = 1\n",
        "def load_ft(words_to_load):\n",
        "    with open('/content/drive/My Drive/Colab Notebooks/wiki-news-300d-1M.vec') as f:\n",
        "        loaded_embeddings = np.zeros((words_to_load+2, 300))\n",
        "        token2id = {}\n",
        "        token2id['<pad>'] = PAD_IDX \n",
        "        token2id['<unk>'] = UNK_IDX\n",
        "        id2token = []\n",
        "        \n",
        "        for i, line in enumerate(f):\n",
        "            if i >= words_to_load: \n",
        "                break\n",
        "            s = line.split()\n",
        "            loaded_embeddings[i+2, :] = np.asarray(s[1:])\n",
        "            id2token.append(s[0])\n",
        "            token2id[s[0]] = i+2\n",
        "    id2token = ['<pad>', '<unk>'] + id2token\n",
        "    return token2id, id2token, loaded_embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SkWI_n-YWcG3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# call load_ft to load fast_text embedding\n",
        "token2id, id2token,ft_emb = load_ft(500000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "96cb8Dj4Eszc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "token2id['<unk>']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O6Tha3k-E3uo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "token2id['UNK']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dmUSwlKmEn45",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "token2id['unk']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_FtSDKLnEj9p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "token2id['UNK']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cD2xhhkmWf0B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def df2idx(fname):\n",
        "    df = pd.read_csv(fname, sep=\"\\t\", index_col=False )\n",
        "    # change the label to numerical value\n",
        "    df.loc[df['label'] == 'entailment', 'label'] = 0\n",
        "    df.loc[df['label'] == 'contradiction', 'label'] = 1\n",
        "    df.loc[df['label'] == 'neutral', 'label'] = 2\n",
        "    # convert token to idx\n",
        "    df['sent1_idx']  = df.apply (lambda row:[token2id[token] if token in token2id else token2id['UNK'] for token in row.sentence1.split()],axis=1)\n",
        "    df['sent2_idx']  = df.apply (lambda row:[token2id[token] if token in token2id else token2id['UNK'] for token in row.sentence2.split()],axis=1)\n",
        "    # convert df to data list and label list\n",
        "    indexed_data = list(zip(df.sent1_idx, df.sent2_idx))\n",
        "    label = np.array(df.label)\n",
        "    return indexed_data, label, df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ON3wj7f8W0jC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "val_data, val_targets, val_df = df2idx(\"/content/snli_val.tsv\")\n",
        "train_data, train_targets, train_df = df2idx(\"/content/snli_train.tsv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "amHVIXIrJCth",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Get a subset of training set to calculate the training accuracy\n",
        "import random\n",
        "subset_idx = random.sample(range(100000), 10000)\n",
        "subset_train_data = [train_data[i] for i in subset_idx]\n",
        "subset_train_targets = [train_targets[i] for i in subset_idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xFyPgQbqm7Dq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Get the max length of sentence in training set\n",
        "print('The max length of sentence 1 is {}'.format(max([len(train_data[i][0]) for i in range(len(train_data))])))\n",
        "print('The max length of sentence 2 is {}'.format(max([len(train_data[i][1]) for i in range(len(train_data))])))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5KEEUgD-XY4l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MAX_SENTENCE1_LENGTH = 82\n",
        "MAX_SENTENCE2_LENGTH = 41\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class NewsGroupDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
        "    Note that this class inherits torch.utils.data.Dataset\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, data_list, target_list):\n",
        "        \"\"\"\n",
        "        @param data_list: list of newsgroup tokens \n",
        "        @param target_list: list of newsgroup targets \n",
        "\n",
        "        \"\"\"\n",
        "        self.data_list = data_list\n",
        "        self.target_list = target_list\n",
        "        assert (len(self.data_list) == len(self.target_list))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "        \n",
        "    def __getitem__(self, key):\n",
        "        \"\"\"\n",
        "        Triggered when you call dataset[i]\n",
        "        \"\"\"\n",
        "        \n",
        "        token1_idx = self.data_list[key][0][:MAX_SENTENCE1_LENGTH]\n",
        "        token2_idx = self.data_list[key][1][:MAX_SENTENCE2_LENGTH]\n",
        "        label = self.target_list[key]\n",
        "        return [token1_idx, token2_idx, len(token1_idx),len(token2_idx),label]\n",
        "\n",
        "def newsgroup_collate_func(batch):\n",
        "    \"\"\"\n",
        "    Customized function for DataLoader that dynamically pads the batch so that all \n",
        "    data have the same length\n",
        "    \"\"\"\n",
        "    token1_data_list = []\n",
        "    token2_data_list = []\n",
        "    label_list = []\n",
        "    token1_length_list = []\n",
        "    token2_length_list = []\n",
        "    #print(\"collate batch: \", batch[0][0])\n",
        "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
        "    for datum in batch:\n",
        "        label_list.append(datum[4])\n",
        "        token1_length_list.append(datum[2])\n",
        "        token2_length_list.append(datum[3])\n",
        "    # padding\n",
        "    for datum in batch:\n",
        "        token1_padded_vec = np.pad(np.array(datum[0]), \n",
        "                                pad_width=((0,MAX_SENTENCE1_LENGTH-datum[2])), \n",
        "                                mode=\"constant\", constant_values=0)\n",
        "        token2_padded_vec = np.pad(np.array(datum[1]), \n",
        "                                pad_width=((0,MAX_SENTENCE2_LENGTH-datum[3])), \n",
        "                                mode=\"constant\", constant_values=0)\n",
        "        token1_data_list.append(token1_padded_vec)\n",
        "        token2_data_list.append(token2_padded_vec)\n",
        "    return [torch.from_numpy(np.array(token1_data_list)), torch.LongTensor(token1_length_list),\n",
        "            torch.from_numpy(np.array(token2_data_list)), torch.LongTensor(token2_length_list),\n",
        "            torch.LongTensor(label_list)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h2f10G1BXhED",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "val_dataset = NewsGroupDataset(val_data, val_targets)\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=newsgroup_collate_func,\n",
        "                                           shuffle=True)\n",
        "\n",
        "train_dataset =  NewsGroupDataset(train_data, train_targets)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=newsgroup_collate_func,\n",
        "                                           shuffle=True)\n",
        "subset_train_dataset =  NewsGroupDataset(subset_train_data, subset_train_targets)\n",
        "subset_train_loader = torch.utils.data.DataLoader(dataset=subset_train_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=newsgroup_collate_func,\n",
        "                                           shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ifq0LuWap3FD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, hidden_size, num_layers, num_classes, pre_trained_emb):\n",
        "        # RNN Accepts the following hyperparams:\n",
        "        # hidden_size: Hidden Size of layer in RNN\n",
        "        # num_layers: number of layers in RNN\n",
        "        # num_classes: number of output classes\n",
        "        # pre_trained_emb : pre_trained embedding matrix. The shape of it can provide the embedding size and vocabulay size\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
        "        # embedding module\n",
        "        self.embedding = nn.Embedding(pre_trained_emb.shape[0], pre_trained_emb.shape[1], padding_idx=PAD_IDX)\n",
        "        # create bi-directional GRU in pytorch(batch_first: the first dim is batch, 2nd is sequence dim, 3rd is embedding dim)\n",
        "        self.rnn = nn.GRU(pre_trained_emb.shape[1], hidden_size,num_layers, bidirectional=True, batch_first = True) \n",
        "        # create decoder layer \n",
        "        self.linear1 = nn.Linear(hidden_size*4,hidden_size )\n",
        "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def init_weights(self, is_static=True):\n",
        "        self.embedding.weight = nn.Parameter(torch.from_numpy(pre_trained_emb).float())\n",
        "        if is_static:\n",
        "            self.embedding.weight.requires_grad = False\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        # Function initializes the activation of recurrent neural net at timestep 0\n",
        "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
        "        hidden = torch.randn(2 * self.num_layers, batch_size, self.hidden_size)\n",
        "        \n",
        "        return hidden\n",
        "\n",
        "    def forward(self, token1_data, token1_lengths, token2_data, token2_lengths):\n",
        "        # reset hidden state\n",
        "\n",
        "        batch_size, token1_seq_len = token1_data.size()\n",
        "        token2_seq_len = token2_data.size()[1]\n",
        "#### main part of RNN ###########################\n",
        "        self.hidden = self.init_hidden(batch_size)\n",
        "        #get the sorted index based on sentent length\n",
        "        _, token1_idx_sort = torch.sort(token1_lengths, dim=0, descending=True)\n",
        "        _, token1_idx_unsort = torch.sort(token1_idx_sort, dim=0)\n",
        "        token1_lengths = token1_lengths[token1_idx_sort]\n",
        "        _, token2_idx_sort = torch.sort(token2_lengths, dim=0, descending=True)\n",
        "        _, token2_idx_unsort = torch.sort(token2_idx_sort, dim=0)\n",
        "        token2_lengths = token2_lengths[token2_idx_sort]\n",
        "        # Sort input data\n",
        "        token1_rnn = token1_data.index_select(0, token1_idx_sort)\n",
        "        token2_rnn = token2_data.index_select(0, token2_idx_sort)\n",
        "        \n",
        "        # get embedding of two sentences\n",
        "        embed_sent1 = self.embedding(token1_rnn)\n",
        "        embed_sent2 = self.embedding(token2_rnn)\n",
        "        \n",
        "        # pack padded sequence\n",
        "        # transform the tensor in pytorch into the padded sequence . pytorch want the sequence in the descending order\n",
        "        embed_sent1 = torch.nn.utils.rnn.pack_padded_sequence(embed_sent1, token1_lengths, batch_first=True)\n",
        "        embed_sent2 = torch.nn.utils.rnn.pack_padded_sequence(embed_sent2, token2_lengths, batch_first=True)\n",
        "        use_cuda = True\n",
        "        if use_cuda and torch.cuda.is_available():\n",
        "            self.hidden = self.hidden.cuda()\n",
        "            \n",
        "        # fprop though RNN # the rnn_out varaible is size of batch size by the sequence length by the hidden dimension\n",
        "#         rnn_out1, _ = self.rnn(embed_sent1, self.hidden) \n",
        "#         rnn_out2, _ = self.rnn(embed_sent2, self.hidden) \n",
        "        _, hidden_out1 = self.rnn(embed_sent1, self.hidden) \n",
        "        _, hidden_out2 = self.rnn(embed_sent2, self.hidden) \n",
        "        hidden_out1 = torch.cat((hidden_out1[0], hidden_out1[1]),dim = 1)\n",
        "        hidden_out2 = torch.cat((hidden_out2[0], hidden_out2[1]),dim = 1)\n",
        "        #unsort \n",
        "        hidden_out1 = hidden_out1.index_select(0, token1_idx_unsort)\n",
        "        hidden_out2 = hidden_out2.index_select(0, token2_idx_unsort)\n",
        "        # undo packing\n",
        "#         rnn_out1, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out1, batch_first=True)\n",
        "#         rnn_out2, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out2, batch_first=True)\n",
        "#         #unsort\n",
        "#         rnn_out1 = rnn_out1.index_select(0, token1_idx_unsort)\n",
        "#         rnn_out2 = rnn_out2.index_select(0, token2_idx_unsort)\n",
        "        # concatenate two encoded sentences\n",
        "        out_cat = torch.cat((hidden_out1, hidden_out2), dim = 1)\n",
        "        # sum hidden activations of RNN across time\n",
        "        #out_cat = torch.sum(out_cat, dim=1)\n",
        "####### main part #########################\n",
        "        hidden1 = self.linear1(out_cat)\n",
        "        hidden1 = self.relu(hidden1)\n",
        "        out = self.linear2(hidden1)\n",
        "        preds = F.log_softmax(out, 1)\n",
        "        return preds\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XcYDyy2lXz61",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_model(loader, model):\n",
        "    \"\"\"\n",
        "    Help function that tests the model's performance on a dataset\n",
        "    @param: loader - data loader for the dataset to test against\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    for data1, lengths1,data2, lengths2, labels in loader:\n",
        "#         data1 = Variable(data1)  \n",
        "#         lengths1 = Variable(lengths1)\n",
        "#         data2 = Variable(data2)  \n",
        "#         lengths2 = Variable(lengths2)# Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
        "#         labels = Variable(labels)\n",
        "        if use_cuda and torch.cuda.is_available():\n",
        "            data1 = data1.cuda()\n",
        "            lengths1  = lengths1.cuda()\n",
        "            data2 = data2.cuda()\n",
        "            lengths2  = lengths2.cuda()\n",
        "            labels = labels.cuda()\n",
        "        data1_batch, lengths1_batch,data2_batch, lengths2_batch, label_batch = data1, lengths1, data2, lengths2,labels\n",
        "        outputs =model(data1_batch, lengths1_batch,data2_batch, lengths2_batch)\n",
        "        predicted = outputs.max(1, keepdim=True)[1]\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
        "    return (100 * correct / total)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WThxJQEqNv5T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(loader, model):\n",
        "  criterion = torch.nn.NLLLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  total_step = len(loader)\n",
        "  #train_loss_ls = []\n",
        "  val_acc_ls = []\n",
        "  train_acc_ls = []\n",
        "  for epoch in range(num_epochs):\n",
        "      #loss_batch = []\n",
        "      for i, (data1, lengths1, data2, lengths2, labels) in enumerate(loader):\n",
        "  #         data1 = Variable(data1)  \n",
        "  #         lengths1 = Variable(lengths1)\n",
        "  #         data2 = Variable(data2)  \n",
        "  #         lengths2 = Variable(lengths2)# Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
        "  #         labels = Variable(labels)\n",
        "          if use_cuda and torch.cuda.is_available():\n",
        "              data1 = data1.cuda()\n",
        "              lengths1  = lengths1.cuda()\n",
        "              data2 = data2.cuda()\n",
        "              lengths2  = lengths2.cuda()\n",
        "              labels = labels.cuda()\n",
        "          model.train()\n",
        "          optimizer.zero_grad()\n",
        "          # Forward pass\n",
        "          outputs = model(data1, lengths1, data2, lengths2)\n",
        "          predicted = outputs.max(1, keepdim=True)[1]\n",
        "          loss = criterion(outputs, labels)\n",
        "          #loss_batch.append(loss.item())\n",
        "          # Backward and optimize\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          # validate every 100 iterations\n",
        "          if i > 0 and i % 400 == 0:\n",
        "              # validate\n",
        "              #train_loss = loss_batch[i]\n",
        "              val_acc = test_model(val_loader, model)\n",
        "              train_acc = test_model(subset_train_loader, model)\n",
        "              train_acc_ls.append(train_acc)\n",
        "              #train_loss_ls.append(train_loss)\n",
        "              val_acc_ls.append(val_acc)\n",
        "              print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}, Training Acc: {}'.format(\n",
        "                         epoch+1, num_epochs, i+1, len(loader), val_acc, train_acc))\n",
        "  #torch.save(model_object.state_dict(), 'params_{}.pkl'.format())\n",
        "#model_object.load_state_dict(torch.load('params.pkl'))\n",
        "  return  val_acc_ls, train_acc_ls\n",
        "  #return train_loss_ls, val_acc_ls, train_acc_ls\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VEu65s4CunOg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2. RNN Tuning"
      ]
    },
    {
      "metadata": {
        "id": "psgj_P4mpEqN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.1 Hidden Size Tuning for RNN\n",
        "- Ways of interacting two sentences: Concatenation \n",
        "- Learning_rate 3e-4\n",
        "- Number of Epochs: 10\n",
        "- Embedding Weights: Freeze All\n",
        "- hidden size list (50, 100, 200, 300, 400 )"
      ]
    },
    {
      "metadata": {
        "id": "QRtK9XbbiqjV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 3e-4\n",
        "num_epochs = 10 # number epoch to train\n",
        "use_cuda = True\n",
        "\n",
        "def find_best_hiddensize_RNN(hidden_ls):\n",
        "  performance = {}\n",
        "  for hidden_size in hidden_ls:\n",
        "    print('---------RNN_HIDDEN_SIZE: {}-------------'.format(hidden_size))\n",
        "    model = RNN(hidden_size = hidden_size, num_layers=1, num_classes=3, pre_trained_emb = ft_emb)\n",
        "    use_cuda = True\n",
        "    if use_cuda and torch.cuda.is_available():\n",
        "      model.cuda()\n",
        "     # Criterion and Optimizer\n",
        "    #\n",
        "    \n",
        "    train_loss, val_acc = train_model(train_loader,model)\n",
        "    performance['hidden_size_{}'.format(hidden_size)] = (train_loss, val_acc)\n",
        "    torch.save(model.state_dict(), 'RNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "  return performance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nRsz_UEsp4QP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Can call this function directly while it takes some time. \n",
        "# Since the runtime on google colab is often disconnected for a long-time run, I chose to run them one by one just in case. \n",
        "\n",
        "hidden_ls = [50,100,200,300,400]\n",
        "hidden_record = find_best_hiddensize_RNN(hidden_ls)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TW1LXbIV_vK1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 50\n",
        "model = RNN(hidden_size = hidden_size, num_layers=1, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "val_acc,train_acc = train_model(train_loader,model)\n",
        "torch.save(model.state_dict(), 'RNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "trainacc_performance['hidden_size_{}'.format(hidden_size)] = (val_acc,train_acc)\n",
        "!cp 'RNN_hidden_size_50.pkl' 'drive/My Drive/Colab Notebooks/RNN_hidden_size_50.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C3QoCQ3MVRD-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "performance = {}\n",
        "hidden_size = 50\n",
        "model = RNN(hidden_size = hidden_size, num_layers=1, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "train_loss, val_acc = train_model(train_loader,model)\n",
        "torch.save(model.state_dict(), 'RNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "performance['hidden_size_{}'.format(hidden_size)] = (train_loss, val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7-KacFq8K3wg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trainacc_performance = {}\n",
        "hidden_size = 100\n",
        "model = RNN(hidden_size = hidden_size, num_layers=1, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "train_loss, val_acc,train_acc = train_model(train_loader,model)\n",
        "torch.save(model.state_dict(), 'RNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "trainacc_performance['hidden_size_{}'.format(hidden_size)] = (train_loss, val_acc,train_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-v4KfqFSdhEk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp 'RNN_hidden_size_100.pkl' 'drive/My Drive/Colab Notebooks/RNN_hidden_size_100.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hBQeHCbOWoN8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#performance = {}\n",
        "hidden_size = 100\n",
        "model = RNN(hidden_size = hidden_size, num_layers=1, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "train_loss, val_acc = train_model(train_loader,model)\n",
        "torch.save(model.state_dict(), 'RNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "performance['hidden_size_{}'.format(hidden_size)] = (train_loss, val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mAvb_DoxeB2n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 200\n",
        "model = RNN(hidden_size = hidden_size, num_layers=1, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "train_loss, val_acc,train_acc = train_model(train_loader,model)\n",
        "torch.save(model.state_dict(), 'RNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "trainacc_performance['hidden_size_{}'.format(hidden_size)] = (train_loss, val_acc,train_acc)\n",
        "!cp 'RNN_hidden_size_200.pkl' 'drive/My Drive/Colab Notebooks/RNN_hidden_size_200.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tSKqC6llXDJu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 200\n",
        "model = RNN(hidden_size = hidden_size, num_layers=1, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "train_loss, val_acc = train_model(train_loader,model)\n",
        "torch.save(model.state_dict(), 'RNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "performance['hidden_size_{}'.format(hidden_size)] = (train_loss, val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PEaEqORFeO56",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 300\n",
        "model = RNN(hidden_size = hidden_size, num_layers=1, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "val_acc,train_acc = train_model(train_loader,model)\n",
        "torch.save(model.state_dict(), 'RNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "trainacc_performance['hidden_size_{}'.format(hidden_size)] = (val_acc,train_acc)\n",
        "!cp 'RNN_hidden_size_300.pkl' 'drive/My Drive/Colab Notebooks/RNN_hidden_size_300.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cWQGjyDJXHmk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 300\n",
        "model = RNN(hidden_size = hidden_size, num_layers=1, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "train_loss, val_acc = train_model(train_loader,model)\n",
        "torch.save(model.state_dict(), 'RNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "performance['hidden_size_{}'.format(hidden_size)] = (train_loss, val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gA5hp5pbnoDN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 400\n",
        "model = RNN(hidden_size = hidden_size, num_layers=1, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "val_acc,train_acc = train_model(train_loader,model)\n",
        "torch.save(model.state_dict(), 'RNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "trainacc_performance['hidden_size_{}'.format(hidden_size)] = (val_acc,train_acc)\n",
        "!cp 'RNN_hidden_size_400.pkl' 'drive/My Drive/Colab Notebooks/RNN_hidden_size_400.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zKQu5shZZQgK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "f = open(\"rnn_hidden_size_record_new.pkl\",\"wb\")\n",
        "pickle.dump(trainacc_performance,f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Eu1ki6kYZaB9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('rnn_hidden_size_record_new.pkl') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZFYrEiNPpORE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 400\n",
        "model = RNN(hidden_size = hidden_size, num_layers=1, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "train_loss, val_acc = train_model(train_loader,model)\n",
        "torch.save(model.state_dict(), 'RNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "performance['hidden_size_{}'.format(hidden_size)] = (train_loss, val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9gJiaKCN5vJF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('RNN_hidden_size_50.pkl')\n",
        "files.download('RNN_hidden_size_100.pkl') \n",
        "files.download('RNN_hidden_size_200.pkl') \n",
        "files.download('RNN_hidden_size_300.pkl')\n",
        "files.download('RNN_hidden_size_400.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fJL0_xhkO3Qm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.2 Concatenate two encoded sentences with element-wise multiplication for RNN\n",
        "- Instead of concatenation of two encoded sentences, do element-wise multiplication\n",
        "- Freeze all embedding weights\n",
        "- Also tuning hidden size\n",
        "- Hidden size list (100, 200, 300, 400, 800)"
      ]
    },
    {
      "metadata": {
        "id": "2UpeeJRnO2Hp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RNN_mul(nn.Module):\n",
        "    def __init__(self, hidden_size, num_layers, num_classes, pre_trained_emb):\n",
        "        # RNN Accepts the following hyperparams\n",
        "        # hidden_size: Hidden Size of layer in RNN\n",
        "        # num_layers: number of layers in RNN\n",
        "        # num_classes: number of output classes\n",
        "        # pre_trained_emb: pre_trained fast text results\n",
        "        super(RNN_mul, self).__init__()\n",
        "\n",
        "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
        "        # embedding module\n",
        "        self.embedding = nn.Embedding(pre_trained_emb.shape[0], pre_trained_emb.shape[1], padding_idx=PAD_IDX)\n",
        "        # create RNN in pytorch(batch_first: the first dim is batch, 2nd is sequence dim, 3rd is embedding dim)\n",
        "        self.rnn = nn.GRU(pre_trained_emb.shape[1], hidden_size,num_layers, bidirectional=True, batch_first = True) \n",
        "        \n",
        "        self.linear1 = nn.Linear(hidden_size*2,hidden_size )\n",
        "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "    def init_weights(self, is_static=True):\n",
        "        self.embedding.weight = nn.Parameter(torch.from_numpy(pre_trained_emb).float())\n",
        "        if is_static:\n",
        "            self.embedding.weight.requires_grad = False\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        # Function initializes the activation of recurrent neural net at timestep 0\n",
        "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
        "        hidden = torch.randn(2 * self.num_layers, batch_size, self.hidden_size)\n",
        "        \n",
        "        return hidden\n",
        "\n",
        "    def forward(self, token1_data, token1_lengths, token2_data, token2_lengths):\n",
        "        # reset hidden state\n",
        "\n",
        "        batch_size, token1_seq_len = token1_data.size()\n",
        "        token2_seq_len = token2_data.size()[1]\n",
        "#### main part of RNN ###########################\n",
        "        self.hidden = self.init_hidden(batch_size)\n",
        "        #sort \n",
        "        _, token1_idx_sort = torch.sort(token1_lengths, dim=0, descending=True)\n",
        "        _, token1_idx_unsort = torch.sort(token1_idx_sort, dim=0)\n",
        "        token1_lengths = token1_lengths[token1_idx_sort]\n",
        "        _, token2_idx_sort = torch.sort(token2_lengths, dim=0, descending=True)\n",
        "        _, token2_idx_unsort = torch.sort(token2_idx_sort, dim=0)\n",
        "        token2_lengths = token2_lengths[token2_idx_sort]\n",
        "        # Sort x\n",
        "        token1_rnn = token1_data.index_select(0, token1_idx_sort)\n",
        "        token2_rnn = token2_data.index_select(0, token2_idx_sort)\n",
        "        \n",
        "        # get embedding of characters\n",
        "        embed_sent1 = self.embedding(token1_rnn)\n",
        "        embed_sent2 = self.embedding(token2_rnn)\n",
        "        # pretrained_weight is a numpy matrix of shape (num_embeddings, embedding_dim)\n",
        "        #embed.weight = nn.Parameter(torch.from_numpy(pre_trained_emb))\n",
        "        \n",
        "        #embed = m * embed + (1-m) * embed.clone().detch()\n",
        "       # embed.weight.data.copy_(torch.from_numpy(pre_trained_emb))\n",
        "        # pack padded sequence\n",
        "        # transform the tensor in pytorch into the padded sequence . pytorch want the sequence in the descending order\n",
        "        embed_sent1 = torch.nn.utils.rnn.pack_padded_sequence(embed_sent1, token1_lengths, batch_first=True)\n",
        "        embed_sent2 = torch.nn.utils.rnn.pack_padded_sequence(embed_sent2, token2_lengths, batch_first=True)\n",
        "        use_cuda = True\n",
        "        if use_cuda and torch.cuda.is_available():\n",
        "#           embed_sent1 = embed_sent1.cuda()\n",
        "#           embed_sent2 = embed_sent1.cuda()\n",
        "            self.hidden = self.hidden.cuda()\n",
        "            \n",
        "        # fprop though RNN # the rnn_out varaible is size of batch size by the sequence length by the hidden dimension\n",
        "#         rnn_out1, _ = self.rnn(embed_sent1, self.hidden) \n",
        "#         rnn_out2, _ = self.rnn(embed_sent2, self.hidden) \n",
        "        _, hidden_out1 = self.rnn(embed_sent1, self.hidden) \n",
        "        _, hidden_out2 = self.rnn(embed_sent2, self.hidden) \n",
        "        hidden_out1 = torch.cat((hidden_out1[0], hidden_out1[1]),dim = 1)\n",
        "        hidden_out2 = torch.cat((hidden_out2[0], hidden_out2[1]),dim = 1)\n",
        "        #unsort \n",
        "        hidden_out1 = hidden_out1.index_select(0, token1_idx_unsort)\n",
        "        hidden_out2 = hidden_out2.index_select(0, token2_idx_unsort)\n",
        "       \n",
        "        out_cat = torch.mul(hidden_out1, hidden_out2)\n",
        "        # sum hidden activations of RNN across time\n",
        "        #out_cat = torch.sum(out_cat, dim=1)\n",
        "####### main part #########################\n",
        "        hidden1 = self.linear1(out_cat)\n",
        "        hidden1 = self.relu(hidden1)\n",
        "        out = self.linear2(hidden1)\n",
        "        preds = F.log_softmax(out, 1)\n",
        "        return preds\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "svGoqBXPrYT0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rnn_mul_trainacc = {}\n",
        "hidden_size = 300\n",
        "model = RNN_mul(hidden_size = hidden_size, num_layers=1, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "val_acc,train_acc = train_model(train_loader,model)\n",
        "torch.save(model.state_dict(), 'mul_RNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "rnn_mul_trainacc['hidden_size_{}'.format(hidden_size)] = (val_acc,train_acc)\n",
        "!cp 'mul_RNN_hidden_size_300.pkl' 'drive/My Drive/Colab Notebooks/mul_RNN_hidden_size_300.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SHUva3bcO1zu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "record_multiply_rnn = {}\n",
        "hidden_size = 300\n",
        "model_mul = RNN_mul(hidden_size = hidden_size, num_layers=1, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model_mul.cuda()\n",
        "train_loss, val_acc = train_model(train_loader,model_mul)\n",
        "torch.save(model_mul.state_dict(), 'RNN_mul_hidden_size_{}.pkl'.format(hidden_size))\n",
        "record_multiply_rnn['hidden_size_{}'.format(hidden_size)] = (train_loss, val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "It51WpPVm4GX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The validation accuracy of RNN model with multipliation encoded sentences when hidden size 300\n",
        "test_model(val_loader, model_mul)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jybFteuXO1km",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "g = open(\"rnn_mul_hidden_size300_record.pkl\",\"wb\")\n",
        "pkl.dump(record_multiply_rnn,g)\n",
        "g.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LHw6HuYjViWT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('rnn_mul_hidden_size300_record.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9qQhQiY5V01R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp 'RNN_mul_hidden_size_300.pkl' 'drive/My Drive/Colab Notebooks'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KN_idly2rzj2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rnn_mul_trainacc = {}\n",
        "hidden_size = 400\n",
        "model = RNN_mul(hidden_size = hidden_size, num_layers=1, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "val_acc,train_acc = train_model(train_loader,model)\n",
        "torch.save(model.state_dict(), 'mul_RNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "rnn_mul_trainacc['hidden_size_{}'.format(hidden_size)] = (val_acc,train_acc)\n",
        "!cp 'mul_RNN_hidden_size_400.pkl' 'drive/My Drive/Colab Notebooks/mul_RNN_hidden_size_400.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w51dq4rx-XRD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "f = open(\"rnn_mul_hidden_new1.pkl\",\"wb\")\n",
        "pickle.dump(rnn_mul_trainacc,f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LsS_Z3hC-o29",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('rnn_mul_hidden_new1.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tuL9Dfz_-u3z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rnn_mul_trainacc.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LYSB14nPnR4z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 400\n",
        "model_mul = RNN_mul(hidden_size = hidden_size, num_layers=1, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model_mul.cuda()\n",
        "train_loss, val_acc = train_model(train_loader,model_mul)\n",
        "torch.save(model_mul.state_dict(), 'RNN_mul_hidden_size_{}.pkl'.format(hidden_size))\n",
        "record_multiply_rnn['hidden_size_{}'.format(hidden_size)] = (train_loss, val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "scfKpnI60gqT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#val acc of hiddensize 400 with multiplication concat\n",
        "test_model(val_loader, model_mul)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3DVr_Gj2tfH4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "g = open(\"rnn_mul_record2.pkl\",\"wb\")\n",
        "pkl.dump(record_multiply_rnn,g)\n",
        "g.close()\n",
        "from google.colab import files\n",
        "files.download('rnn_mul_record2.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f_bMyEdrtvPQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp 'RNN_mul_hidden_size_400.pkl' 'drive/My Drive/Colab Notebooks'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w2ZRDdf6_F_-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 200\n",
        "model = RNN_mul(hidden_size = hidden_size, num_layers=1, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "val_acc,train_acc = train_model(train_loader,model)\n",
        "torch.save(model.state_dict(), 'mul_RNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "rnn_mul_trainacc['hidden_size_{}'.format(hidden_size)] = (val_acc,train_acc)\n",
        "!cp 'mul_RNN_hidden_size_200.pkl' 'drive/My Drive/Colab Notebooks/mul_RNN_hidden_size_200.pkl'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eYRZt6Fm_SXi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "f = open(\"rnn_mul_hidden_new2.pkl\",\"wb\")\n",
        "pickle.dump(rnn_mul_trainacc,f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S4xREttf_U0z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('rnn_mul_hidden_new2.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tDcQtIww5Xcm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 200\n",
        "model_mul_200= RNN_mul(hidden_size = hidden_size, num_layers=1, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model_mul_200.cuda()\n",
        "train_loss, val_acc = train_model(train_loader,model_mul_200)\n",
        "torch.save(model_mul_200.state_dict(), 'RNN_mul_hidden_size_{}.pkl'.format(hidden_size))\n",
        "record_multiply_rnn['hidden_size_{}'.format(hidden_size)] = (train_loss, val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MljReXprHOtw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#val acc of hiddensize 200 with multiplication concat\n",
        "test_model(val_loader, model_mul_200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A3UzRW6WHTQI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "g = open(\"rnn_mul_record3.pkl\",\"wb\")\n",
        "pkl.dump(record_multiply_rnn,g)\n",
        "g.close()\n",
        "from google.colab import files\n",
        "files.download('rnn_mul_record3.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2diQ7F3mHYuN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp 'RNN_mul_hidden_size_200.pkl' 'drive/My Drive/Colab Notebooks'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E_wxu3YHLqOb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('RNN_mul_hidden_size_200.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fUNogCT2_dL2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 100\n",
        "model = RNN_mul(hidden_size = hidden_size, num_layers=1, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "val_acc,train_acc = train_model(train_loader,model)\n",
        "torch.save(model.state_dict(), 'mul_RNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "rnn_mul_trainacc['hidden_size_{}'.format(hidden_size)] = (val_acc,train_acc)\n",
        "!cp 'mul_RNN_hidden_size_100.pkl' 'drive/My Drive/Colab Notebooks/mul_RNN_hidden_size_100.pkl'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "deHy-C5g_f0W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "f = open(\"rnn_mul_hidden_new3.pkl\",\"wb\")\n",
        "pickle.dump(rnn_mul_trainacc,f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UCSAZb84_nmR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download(\"rnn_mul_hidden_new3.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tmoq8TUrLwWN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 100\n",
        "model_mul_100= RNN_mul(hidden_size = hidden_size, num_layers=1, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model_mul_100.cuda()\n",
        "train_loss, val_acc = train_model(train_loader,model_mul_100)\n",
        "torch.save(model_mul_100.state_dict(), 'RNN_mul_hidden_size_{}.pkl'.format(hidden_size))\n",
        "record_multiply_rnn['hidden_size_{}'.format(hidden_size)] = (train_loss, val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sw4cRjN_anDm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#val acc of hiddensize 100 with multiplication concat\n",
        "test_model(val_loader, model_mul_100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3TsmCNPEcymR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "g = open(\"rnn_mul_record4.pkl\",\"wb\")\n",
        "pkl.dump(record_multiply_rnn,g)\n",
        "g.close()\n",
        "from google.colab import files\n",
        "files.download('rnn_mul_record4.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4krWTCdsc5Cu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp 'RNN_mul_hidden_size_100.pkl' 'drive/My Drive/Colab Notebooks'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2-BoNWXRdFWJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('RNN_mul_hidden_size_100.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HFy_burnWxq_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 3e-4\n",
        "num_epochs = 10 # number epoch to train\n",
        "use_cuda = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xQG-jsKEKyNw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 800\n",
        "model = RNN_mul(hidden_size = hidden_size, num_layers=1, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "val_acc,train_acc = train_model(train_loader,model)\n",
        "torch.save(model.state_dict(), 'mul_RNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "rnn_mul_trainacc['hidden_size_{}'.format(hidden_size)] = (val_acc,train_acc)\n",
        "!cp 'mul_RNN_hidden_size_800.pkl' 'drive/My Drive/Colab Notebooks/mul_RNN_hidden_size_800.pkl'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vJGJJwW__272",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "f = open(\"rnn_mul_hidden_new4.pkl\",\"wb\")\n",
        "pickle.dump(rnn_mul_trainacc,f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-ur8ckgp_4-K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download(\"rnn_mul_hidden_new4.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F1-yJo3wdV53",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "record_mul_800 = {}\n",
        "hidden_size = 800\n",
        "model_mul_800= RNN_mul(hidden_size = hidden_size, num_layers=1, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model_mul_800.cuda()\n",
        "train_loss, val_acc = train_model(train_loader,model_mul_800)\n",
        "torch.save(model_mul_800.state_dict(), 'drive/My Drive/Colab Notebook/RNN_mul_hidden_size_{}.pkl'.format(hidden_size))\n",
        "record_mul_800['hidden_size_{}'.format(hidden_size)] = (train_loss, val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dcy58bwxsbR6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.save(model_mul_800.state_dict(), 'RNN_mul_hidden_size_{}.pkl'.format(hidden_size))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J3FwQzTns5km",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp 'RNN_mul_hidden_size_800.pkl' 'drive/My Drive/Colab Notebooks'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QkNzqm0WtCDW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "record_mul_800['hidden_size_{}'.format(hidden_size)] = (train_loss, val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gfOorD0ttIKY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#val acc of hiddensize 800 with multiplication concat\n",
        "val_acc_rnn_mul800 = test_model(val_loader, model_mul_800)\n",
        "val_acc_rnn_mul800"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n9BN9fk2uiuV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rnn_mul_100 = RNN_mul(hidden_size = 100,  num_layers = 1, num_classes = 3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  rnn_mul_100.cuda()\n",
        "rnn_mul_100.load_state_dict(torch.load('drive/My Drive/Colab Notebooks/RNN_mul_hidden_size_100.pkl'))\n",
        "test_model(val_loader, rnn_mul_100)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u8pGKec0tbK_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "g = open(\"rnn_mul_800_record.pkl\",\"wb\")\n",
        "pkl.dump(record_mul_800,g)\n",
        "g.close()\n",
        "from google.colab import files\n",
        "files.download('rnn_mul_800_record.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QJT4lXnOuaY_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3. CNN Tuning"
      ]
    },
    {
      "metadata": {
        "id": "EdaTnbGtu77H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.1 Hidden Size Tuning for CNN\n",
        "- Ways of interacting two encoded sentences: concatenation\n",
        "- Weights of embedding : freeze all\n",
        "- Kernel size: 3\n",
        "- Hidden size list: (50, 100, 200, 300, 400, 800)"
      ]
    },
    {
      "metadata": {
        "id": "KTpqA_UyNpUb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self,  hidden_size, kernel_size, padding_size, num_layers, num_classes, pre_trained_emb):\n",
        "\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.num_layers, self.hidden_size, self.kernel_size ,self.padding_size= num_layers, hidden_size,kernel_size,padding_size\n",
        "        self.embedding = nn.Embedding(pre_trained_emb.shape[0], pre_trained_emb.shape[1], padding_idx=PAD_IDX)\n",
        "        #emb_size is the size of imput, hidden_size is the size of output. kernel_size is like the window size, \n",
        "        # the kernel size 3 here means read 3 words/chars once\n",
        "        self.conv1 = nn.Conv1d( pre_trained_emb.shape[1], hidden_size, kernel_size, padding=padding_size)\n",
        "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size, padding=padding_size)\n",
        "\n",
        "        self.linear1 = nn.Linear(hidden_size*2, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
        "        #self.maxpooling = nn.MaxPool1d()\n",
        "    def init_weights(self, is_static=True):\n",
        "        self.embedding.weight = nn.Parameter(torch.from_numpy(pre_trained_emb).float())\n",
        "        if is_static:\n",
        "            self.embedding.weight.requires_grad = False\n",
        "            \n",
        "    def forward(self, token1_data, token1_lengths,token2_data, token2_lengths):\n",
        "        batch_size, token1_seq_len = token1_data.size()\n",
        "        _,token2_seq_len = token2_data.size()\n",
        "\n",
        "        embed_sent1 = self.embedding(token1_data)\n",
        "        embed_sent2 = self.embedding(token2_data)\n",
        "        # the convolusional module in pytorch expects the input of size  batch size by the hidden size by the sequence length\n",
        "        hidden_sent1 = self.conv1(embed_sent1.transpose(1,2)).transpose(1,2)\n",
        "        hidden_sent2 = self.conv1(embed_sent2.transpose(1,2)).transpose(1,2)\n",
        "        # relu expect 2-d tensor as input , merging the 0th and 1st dim together\n",
        "        hidden_sent1 = F.relu(hidden_sent1.contiguous().view(-1, hidden_sent1.size(-1))).view(batch_size, token1_seq_len, hidden_sent1.size(-1))\n",
        "        hidden_sent2 = F.relu(hidden_sent2.contiguous().view(-1, hidden_sent2.size(-1))).view(batch_size, token2_seq_len, hidden_sent2.size(-1))\n",
        "        hidden_sent1 = self.conv2(hidden_sent1.transpose(1,2)).transpose(1,2)\n",
        "        hidden_sent2 = self.conv2(hidden_sent2.transpose(1,2)).transpose(1,2)\n",
        "        hidden_sent1 = F.relu(hidden_sent1.contiguous().view(-1, hidden_sent1.size(-1))).view(batch_size, token1_seq_len, hidden_sent1.size(-1))\n",
        "        hidden_sent2 = F.relu(hidden_sent2.contiguous().view(-1, hidden_sent2.size(-1))).view(batch_size, token2_seq_len, hidden_sent2.size(-1))\n",
        "        # max-pooling over time\n",
        "        hidden_sent1 = F.max_pool1d(hidden_sent1.transpose(1,2), kernel_size = token1_seq_len ).transpose(1,2)\n",
        "        hidden_sent2 = F.max_pool1d(hidden_sent2.transpose(1,2), kernel_size = token2_seq_len ).transpose(1,2)\n",
        "        hidden = torch.cat((hidden_sent1, hidden_sent2), dim=2).squeeze()\n",
        "        fc1_out = self.linear1(hidden)\n",
        "        fc1_out = self.relu(fc1_out)\n",
        "        fc2_out = self.linear2(fc1_out)\n",
        "        preds = F.log_softmax(fc2_out, 1)\n",
        "        return preds\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kYjPSGAfY6AG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 3e-4\n",
        "num_epochs = 10 # number epoch to train\n",
        "use_cuda = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yXjB0GxfnMkG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cnn_hidden_trainacc = {}\n",
        "hidden_size = 50\n",
        "model = CNN(hidden_size = hidden_size,  kernel_size = 3,padding_size = 1,num_layers=2, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "val_acc,train_acc = train_model(train_loader,model)\n",
        "torch.save(model.state_dict(), 'new_CNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "cnn_hidden_trainacc['hidden_size_{}'.format(hidden_size)] = (val_acc,train_acc)\n",
        "!cp 'new_CNN_hidden_size_50.pkl' 'drive/My Drive/Colab Notebooks/new_CNN_hidden_size_50.pkl'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wGawmQ032SNM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "f = open(\"cnn_cat_hidden50_trainacc.pkl\",\"wb\")\n",
        "pickle.dump(cnn_hidden_trainacc,f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m9ApxsS82cki",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('cnn_cat_hidden50_trainacc.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xZI7FiiNY2PH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "performance_CNN = {}\n",
        "hidden_size = 50\n",
        "model_CNN = CNN(hidden_size = hidden_size, kernel_size = 3,padding_size = 1, num_layers=2, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model_CNN.cuda()\n",
        "train_loss, val_acc = train_model(train_loader,model_CNN)\n",
        "torch.save(model_CNN.state_dict(), 'CNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "performance_CNN['hidden_size_{}'.format(hidden_size)] = (train_loss, val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ic5MWiMP8C4J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "g = open(\"cnn_hidden_size_record.pkl\",\"wb\")\n",
        "pickle.dump(performance_CNN,g)\n",
        "g.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iH2Aoegy8KTw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('cnn_hidden_size_record.pkl') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fjp7ey0L2lg8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#cnn_hidden_trainacc = {}\n",
        "hidden_size = 100\n",
        "model = CNN(hidden_size = hidden_size,  kernel_size = 3,padding_size = 1,num_layers=2, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "val_acc,train_acc = train_model(train_loader,model)\n",
        "torch.save(model.state_dict(), 'new_CNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "cnn_hidden_trainacc['hidden_size_{}'.format(hidden_size)] = (val_acc,train_acc)\n",
        "!cp 'new_CNN_hidden_size_100.pkl' 'drive/My Drive/Colab Notebooks/new_CNN_hidden_size_100.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gXI9tLnTas43",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#cnn_hidden_trainacc = {}\n",
        "hidden_size = 100\n",
        "model = CNN(hidden_size = hidden_size,  kernel_size = 3,padding_size = 1,num_layers=2, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "val_acc,train_acc = train_model(train_loader,model)\n",
        "#torch.save(model.state_dict(), 'new_CNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "cnn_hidden_trainacc['hidden_size_{}'.format(hidden_size)] = (val_acc,train_acc)\n",
        "#!cp 'new_CNN_hidden_size_100.pkl' 'drive/My Drive/Colab Notebooks/new_CNN_hidden_size_100.pkl'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qUU29Szya0QU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "g = open(\"cnn_hidden100_acc.pkl\",\"wb\")\n",
        "pickle.dump(cnn_hidden_trainacc,g)\n",
        "g.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FHaWJJn5azv5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('cnn_hidden100_acc.pkl') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lLHP2wyRLbhS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cnn_hidden_trainacc.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ESXiiPrw8RC0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "performance_CNN = {}\n",
        "hidden_size = 100\n",
        "model_CNN = CNN(hidden_size = hidden_size, kernel_size = 3,padding_size = 1, num_layers=1, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model_CNN.cuda()\n",
        "train_loss, val_acc = train_model(train_loader,model_CNN)\n",
        "torch.save(model_CNN.state_dict(), 'CNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "performance_CNN['hidden_size_{}'.format(hidden_size)] = (train_loss, val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wLBOb-H_8x9T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('CNN_hidden_size_100.pkl') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KaBhzN0vLl44",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#cnn_hidden_trainacc = {}\n",
        "hidden_size = 200\n",
        "model = CNN(hidden_size = hidden_size,  kernel_size = 3,padding_size = 1,num_layers=2, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "val_acc,train_acc = train_model(train_loader,model)\n",
        "torch.save(model.state_dict(), 'new_CNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "cnn_hidden_trainacc['hidden_size_{}'.format(hidden_size)] = (val_acc,train_acc)\n",
        "!cp 'new_CNN_hidden_size_200.pkl' 'drive/My Drive/Colab Notebooks/new_CNN_hidden_size_200.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iR8FBr-9bW-e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#cnn_hidden_trainacc = {}\n",
        "hidden_size = 200\n",
        "model = CNN(hidden_size = hidden_size,  kernel_size = 3,padding_size = 1,num_layers=2, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "val_acc,train_acc = train_model(train_loader,model)\n",
        "#torch.save(model.state_dict(), 'new_CNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "cnn_hidden_trainacc['hidden_size_{}'.format(hidden_size)] = (val_acc,train_acc)\n",
        "#!cp 'new_CNN_hidden_size_200.pkl' 'drive/My Drive/Colab Notebooks/new_CNN_hidden_size_200.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wfV-zWW3NNuJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cnn_hidden_trainacc = {}\n",
        "cnn_hidden_trainacc['hidden_size_{}'.format(hidden_size)] = (val_acc,train_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iqxTCN4ibr2Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "g = open(\"cnn_hidden200_acc.pkl\",\"wb\")\n",
        "pickle.dump(cnn_hidden_trainacc,g)\n",
        "g.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qvVqnq_JbmGc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('cnn_hidden200_acc.pkl') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DPAXiuCh31YJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 200\n",
        "model_CNN = CNN(hidden_size = hidden_size, kernel_size = 3, padding_size = 1, num_layers=1, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model_CNN.cuda()\n",
        "train_loss, val_acc = train_model(train_loader,model_CNN)\n",
        "torch.save(model_CNN.state_dict(), 'CNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "performance_CNN['hidden_size_{}'.format(hidden_size)] = (train_loss, val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dpXlm-ufZ9CZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cnn_hidden_trainacc = {}\n",
        "hidden_size = 300\n",
        "model = CNN(hidden_size = hidden_size,  kernel_size = 3,padding_size = 1,num_layers=2, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "val_acc,train_acc = train_model(train_loader,model)\n",
        "#torch.save(model.state_dict(), 'new_CNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "cnn_hidden_trainacc['hidden_size_{}'.format(hidden_size)] = (val_acc,train_acc)\n",
        "#!cp 'new_CNN_hidden_size_300.pkl' 'drive/My Drive/Colab Notebooks/new_CNN_hidden_size_300.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "19NkZPgdb3y8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "g = open(\"cnn_hidden300_acc.pkl\",\"wb\")\n",
        "pickle.dump(cnn_hidden_trainacc,g)\n",
        "g.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2-iq9mKjb7mZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('cnn_hidden300_acc.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MMqr4bkTefm2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cnn_hidden_trainacc.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y2vDwd6w0H8E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "performance_CNN = {}\n",
        "hidden_size = 300\n",
        "model_CNN = CNN(hidden_size = hidden_size, kernel_size = 3, padding_size = 1, num_layers=2, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model_CNN.cuda()\n",
        "train_loss, val_acc = train_model(train_loader,model_CNN)\n",
        "torch.save(model_CNN.state_dict(), 'CNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "performance_CNN['hidden_size_{}'.format(hidden_size)] = (train_loss, val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xfbWgZKm1S9X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp 'CNN_hidden_size_300.pkl' 'drive/My Drive/Colab Notebooks/CNN_hidden_size_300.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6WxDvtv0G0jr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_model(val_loader, model_CNN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SCb40zmNpP4x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "performance_CNN = {}\n",
        "hidden_size = 300\n",
        "model_CNN = CNN(hidden_size = hidden_size, kernel_size = 3, padding_size = 1, num_layers=2, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model_CNN.cuda()\n",
        "train_loss, val_acc = train_model(train_loader,model_CNN)\n",
        "torch.save(model_CNN.state_dict(), 'CNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "performance_CNN['hidden_size_{}'.format(hidden_size)] = (train_loss, val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "srco3CFjaPO2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cnn_hidden_trainacc = {}\n",
        "hidden_size = 400\n",
        "model = CNN(hidden_size = hidden_size,  kernel_size = 3,padding_size = 1,num_layers=2, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "val_acc,train_acc = train_model(train_loader,model)\n",
        "torch.save(model.state_dict(), 'new_CNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "cnn_hidden_trainacc['hidden_size_{}'.format(hidden_size)] = (val_acc,train_acc)\n",
        "!cp 'new_CNN_hidden_size_400.pkl' 'drive/My Drive/Colab Notebooks/new_CNN_hidden_size_400.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CifE8XnEsZLc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "g = open(\"cnn_hidden_size400_new.pkl\",\"wb\")\n",
        "pickle.dump(cnn_hidden_trainacc,g)\n",
        "g.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OIIJ6p1Isf7w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('cnn_hidden_size400_new.pkl') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j4oQz4nypUSP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 400\n",
        "model_CNN = CNN(hidden_size = hidden_size, kernel_size = 3, padding_size = 1, num_layers=2, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model_CNN.cuda()\n",
        "train_loss, val_acc = train_model(train_loader,model_CNN)\n",
        "torch.save(model_CNN.state_dict(), 'CNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "performance_CNN['hidden_size_{}'.format(hidden_size)] = (train_loss, val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Sh6NeTeaYNw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#cnn_hidden_trainacc = {}\n",
        "hidden_size = 800\n",
        "model = CNN(hidden_size = hidden_size,  kernel_size = 3,padding_size = 1,num_layers=2, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "val_acc,train_acc = train_model(train_loader,model)\n",
        "#torch.save(model.state_dict(), 'new_CNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "cnn_hidden_trainacc['hidden_size_{}'.format(hidden_size)] = (val_acc,train_acc)\n",
        "#!cp 'new_CNN_hidden_size_800.pkl' 'drive/My Drive/Colab Notebooks/new_CNN_hidden_size_800.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hOPPO-Zd1Rsk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cnn_hidden_trainacc.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cH9cLvtJxb5P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "g = open(\"cnn_hidden_size800_new.pkl\",\"wb\")\n",
        "pickle.dump(cnn_hidden_trainacc,g)\n",
        "g.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3H2mCZaGb2sP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "g = open(\"cnn_hidden_size800_new.pkl\",\"wb\")\n",
        "pickle.dump(cnn_hidden_trainacc,g)\n",
        "g.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GLPHYkGXb7za",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('cnn_hidden_size800_new.pkl') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "itZgIM96NgqY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 800\n",
        "model_CNN = CNN(hidden_size = hidden_size, kernel_size = 3, padding_size = 1, num_layers=2, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model_CNN.cuda()\n",
        "train_loss, val_acc = train_model(train_loader,model_CNN)\n",
        "torch.save(model_CNN.state_dict(), 'CNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "performance_CNN['hidden_size_{}'.format(hidden_size)] = (train_loss, val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9YbpZMAWKyRk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_cnn = CNN(hidden_size = 300, kernel_size = 3, padding_size = 1, num_layers = 2, num_classes = 3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model_cnn.cuda()\n",
        "model_cnn.load_state_dict(torch.load('CNN_hidden_size_300.pkl'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RRnz2MmoLB6t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# VAL ACC for CNN with hidden_size 300 and kernel_size 3 :65.6\n",
        "test_model(val_loader, model_cnn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VlBMdqjzL7ey",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# VAL ACC for CNN with hidden_size 400 and kernel_size 3: 64.9\n",
        "model_cnn = CNN(hidden_size = 400, kernel_size = 3, padding_size = 1, num_layers = 2, num_classes = 3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model_cnn.cuda()\n",
        "model_cnn.load_state_dict(torch.load('CNN_hidden_size_400.pkl'))\n",
        "test_model(val_loader, model_cnn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oCNKhHhEkC8A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# VAL ACC for CNN with hidden_size 800 and kernel_size 3: 65.0\n",
        "model_cnn = CNN(hidden_size = 800, kernel_size = 3, padding_size = 1, num_layers = 2, num_classes = 3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model_cnn.cuda()\n",
        "model_cnn.load_state_dict(torch.load('CNN_hidden_size_800.pkl'))\n",
        "test_model(val_loader, model_cnn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tzk7liqFkE4Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "g = open(\"cnn_hidden_size_record3.pkl\",\"wb\")\n",
        "pkl.dump(performance_CNN,g)\n",
        "g.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hZjt6AgxkSBY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('cnn_hidden_size_record3.pkl') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "imc3ok0XnE7x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.2  Kernel Size Tuning for CNN\n",
        "- Hidden size 400\n",
        "- Kernel size list (3, 5, 7)"
      ]
    },
    {
      "metadata": {
        "id": "_sB07T2Sf0Br",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cnn_kernel_trainacc = {}\n",
        "kernel_size = 5\n",
        "model = CNN(hidden_size = 400,  kernel_size = kernel_size,padding_size = 2,num_layers=2, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "val_acc,train_acc = train_model(train_loader,model)\n",
        "torch.save(model.state_dict(), 'new_CNN_kernel_size_{}.pkl'.format(hidden_size))\n",
        "cnn_kernel_trainacc['kernel_size_{}'.format(kernel_size)] = (val_acc,train_acc)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dyjq4a4Ss9L3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "g = open(\"cnn_kernel5_acc.pkl\",\"wb\")\n",
        "pkl.dump(cnn_kernel_trainacc,g)\n",
        "g.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xSgE31NKtImK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('cnn_kernel5_acc.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XPtEQyYsNqcq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp 'new_CNN_kernel_size_400.pkl' 'drive/My Drive/Colab Notebooks/new_CNN_kernel_size_5.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TGKM3tHgmq4u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cnn_kernel_record = {}\n",
        "kernel_size = 5\n",
        "model_CNN = CNN(hidden_size = 400, kernel_size = kernel_size, padding_size = 2,num_layers=2, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model_CNN.cuda()\n",
        "train_loss, val_acc = train_model(train_loader,model_CNN)\n",
        "torch.save(model_CNN.state_dict(), 'CNN_kernel_size_{}.pkl'.format(kernel_size))\n",
        "cnn_kernel_record['kernel_size_{}'.format(kernel_size)] = (train_loss, val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G-NBoHBD7Rsu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# VAL ACC for CNN with hidden_size 400 and kernel_size 5: 62\n",
        "model_cnn = CNN(hidden_size = 400, kernel_size = 5, padding_size = 2, num_layers = 2, num_classes = 3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model_cnn.cuda()\n",
        "model_cnn.load_state_dict(torch.load('CNN_kernel_size_5.pkl'))\n",
        "test_model(val_loader, model_cnn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h8-DKnOmN66O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#cnn_kernel_trainacc = {}\n",
        "kernel_size = 7\n",
        "model = CNN(hidden_size = 400,  kernel_size = kernel_size,padding_size = 3,num_layers=2, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "val_acc,train_acc = train_model(train_loader,model)\n",
        "torch.save(model.state_dict(), 'new_CNN_kernel_size_{}.pkl'.format(kernel_size))\n",
        "cnn_kernel_trainacc['kernel_size_{}'.format(kernel_size)] = (val_acc,train_acc)\n",
        "!cp 'new_CNN_kernel_size_7.pkl' 'drive/My Drive/Colab Notebooks/new_CNN_kernel_size_7.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EvbcF_ODOOKf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "g = open(\"cnn_kernel7_acc.pkl\",\"wb\")\n",
        "pkl.dump(cnn_kernel_trainacc,g)\n",
        "g.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DOGnkCQOOQmX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('cnn_kernel7_acc.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4q_2iUZdcplj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cnn_kernel_record1 = {}\n",
        "kernel_size = 7\n",
        "model_CNN = CNN(hidden_size = 400, kernel_size = kernel_size, padding_size = 3,num_layers=2, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model_CNN.cuda()\n",
        "train_loss, val_acc = train_model(train_loader,model_CNN)\n",
        "torch.save(model_CNN.state_dict(), 'CNN_kernel_size_{}.pkl'.format(kernel_size))\n",
        "cnn_kernel_record1['kernel_size_{}'.format(kernel_size)] = (train_loss, val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2lCMPq7rwNb2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp 'CNN_kernel_size_7.pkl' 'drive/My Drive/Colab Notebooks/CNN_kernel_size_7.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uksx4gHUEsa0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_model(val_loader, model_CNN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zj4DxDsVE6pC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "g = open(\"cnn_kernel_size_record1.pkl\",\"wb\")\n",
        "pkl.dump(cnn_kernel_record1,g)\n",
        "g.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U5iglQWzFBem",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('cnn_kernel_size_record1.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9SargazNK7VT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# VAL ACC for CNN with hidden_size 400 and kernel_size 7: 65\n",
        "model_cnn = CNN(hidden_size = 800, kernel_size = 3, padding_size = 1, num_layers = 2, num_classes = 3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model_cnn.cuda()\n",
        "model_cnn.load_state_dict(torch.load('drive/My Drive/Colab Notebooks/CNN_hidden_size_800.pkl'))\n",
        "test_model(val_loader, model_cnn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_hgi5ZKLZUGN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# VAL ACC for CNN with hidden_size 400 and kernel_size 5: 62\n",
        "model_cnn = CNN(hidden_size = 400, kernel_size = 5, padding_size = 2, num_layers = 2, num_classes = 3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model_cnn.cuda()\n",
        "model_cnn.load_state_dict(torch.load('CNN_kernel_size_5.pkl'))\n",
        "test_model(val_loader, model_cnn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dDTTpIBlmqro",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp 'CNN_hidden_size_800.pkl' 'drive/My Drive/Colab Notebooks'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BH4an28l9h5T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "g = open(\"cnn_kernel_size_record.pkl\",\"wb\")\n",
        "pkl.dump(cnn_kernel_record,g)\n",
        "g.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V3-rrRhc9xCg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('cnn_kernel_size_record.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4piixm8hLIl-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.3 Concatenate two encoded sentences with element-wise multiplication for CNN\n",
        "- Kernel size: 3\n",
        "- Hidden size list: (200, 300, 400)"
      ]
    },
    {
      "metadata": {
        "id": "gFhFU-C-LiSE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CNN_mul(nn.Module):\n",
        "    def __init__(self,  hidden_size, kernel_size, padding_size, num_layers, num_classes, pre_trained_emb):\n",
        "\n",
        "        super(CNN_mul, self).__init__()\n",
        "\n",
        "        self.num_layers, self.hidden_size, self.kernel_size ,self.padding_size= num_layers, hidden_size,kernel_size,padding_size\n",
        "        self.embedding = nn.Embedding(pre_trained_emb.shape[0], pre_trained_emb.shape[1], padding_idx=PAD_IDX)\n",
        "        #emb_size is the size of imput, hidden_size is the size of output. kernel_size is like the window size, \n",
        "        # the kernel size 3 here means read 3 words/chars once\n",
        "        self.conv1 = nn.Conv1d( pre_trained_emb.shape[1], hidden_size, kernel_size, padding=padding_size)\n",
        "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size, padding=padding_size)\n",
        "\n",
        "        self.linear1 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
        "        #self.maxpooling = nn.MaxPool1d()\n",
        "    def init_weights(self, is_static=True):\n",
        "        self.embedding.weight = nn.Parameter(torch.from_numpy(pre_trained_emb).float())\n",
        "        if is_static:\n",
        "            self.embedding.weight.requires_grad = False\n",
        "            \n",
        "    def forward(self, token1_data, token1_lengths,token2_data, token2_lengths):\n",
        "        batch_size, token1_seq_len = token1_data.size()\n",
        "        _,token2_seq_len = token2_data.size()\n",
        "\n",
        "        embed_sent1 = self.embedding(token1_data)\n",
        "        embed_sent2 = self.embedding(token2_data)\n",
        "        # the convolusional module in pytorch expects the input of size  batch size by the hidden size by the sequence length\n",
        "        hidden_sent1 = self.conv1(embed_sent1.transpose(1,2)).transpose(1,2)\n",
        "        hidden_sent2 = self.conv1(embed_sent2.transpose(1,2)).transpose(1,2)\n",
        "        # relu expect 2-d tensor as input , merging the 0th and 1st dim together\n",
        "        hidden_sent1 = F.relu(hidden_sent1.contiguous().view(-1, hidden_sent1.size(-1))).view(batch_size, token1_seq_len, hidden_sent1.size(-1))\n",
        "        hidden_sent2 = F.relu(hidden_sent2.contiguous().view(-1, hidden_sent2.size(-1))).view(batch_size, token2_seq_len, hidden_sent2.size(-1))\n",
        "        hidden_sent1 = self.conv2(hidden_sent1.transpose(1,2)).transpose(1,2)\n",
        "        hidden_sent2 = self.conv2(hidden_sent2.transpose(1,2)).transpose(1,2)\n",
        "        hidden_sent1 = F.relu(hidden_sent1.contiguous().view(-1, hidden_sent1.size(-1))).view(batch_size, token1_seq_len, hidden_sent1.size(-1))\n",
        "        hidden_sent2 = F.relu(hidden_sent2.contiguous().view(-1, hidden_sent2.size(-1))).view(batch_size, token2_seq_len, hidden_sent2.size(-1))\n",
        "        # max-pooling over time\n",
        "        hidden_sent1 = F.max_pool1d(hidden_sent1.transpose(1,2), kernel_size = token1_seq_len ).transpose(1,2)\n",
        "        hidden_sent2 = F.max_pool1d(hidden_sent2.transpose(1,2), kernel_size = token2_seq_len ).transpose(1,2)\n",
        "        hidden = torch.mul(hidden_sent1, hidden_sent2).squeeze()\n",
        "        fc1_out = self.linear1(hidden)\n",
        "        fc1_out = self.relu(fc1_out)\n",
        "        fc2_out = self.linear2(fc1_out)\n",
        "        preds = F.log_softmax(fc2_out, 1)\n",
        "        return preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WEjqSqLsMQbi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cnn_mul_hidden_trainacc = {}\n",
        "hidden_size = 200\n",
        "model = CNN_mul(hidden_size = hidden_size,  kernel_size = 3,padding_size = 1,num_layers=2, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "val_acc,train_acc = train_model(train_loader,model)\n",
        "#torch.save(model.state_dict(), 'new_CNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "cnn_mul_hidden_trainacc['hidden_size_{}'.format(hidden_size)] = (val_acc,train_acc)\n",
        "#!cp 'new_CNN_hidden_size_800.pkl' 'drive/My Drive/Colab Notebooks/new_CNN_hidden_size_800.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "onPs8Zt7MZ_s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "g = open(\"cnn_mul_hidden200_acc.pkl\",\"wb\")\n",
        "pickle.dump(cnn_mul_hidden_trainacc,g)\n",
        "g.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TRccju8OMkpG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('cnn_mul_hidden200_acc.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AlxTn4RpMhqD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# VAL ACC for CNN with hidden_size 200 and kernel_size 3:\n",
        "cnn_mul_record = {}\n",
        "hidden_size = 200\n",
        "cnn_mul_200 = CNN_mul(hidden_size = hidden_size ,kernel_size = 3, padding_size = 1, num_layers = 2, num_classes = 3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  cnn_mul_200.cuda()\n",
        "train_loss, val_acc = train_model(train_loader,cnn_mul_200)\n",
        "torch.save(cnn_mul_200.state_dict(), 'CNN_mul_hidden_size_{}.pkl'.format(hidden_size))\n",
        "cnn_mul_record['hidden_size_{}'.format(hidden_size)] = (train_loss, val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eMZyBkxhOJBF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_model(val_loader, cnn_mul_200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xVbEpAOpa4ua",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#cnn_mul_hidden_trainacc = {}\n",
        "hidden_size = 300\n",
        "model = CNN_mul(hidden_size = hidden_size,  kernel_size = 3,padding_size = 1,num_layers=2, num_classes=3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "val_acc,train_acc = train_model(train_loader,model)\n",
        "#torch.save(model.state_dict(), 'new_CNN_hidden_size_{}.pkl'.format(hidden_size))\n",
        "cnn_mul_hidden_trainacc['hidden_size_{}'.format(hidden_size)] = (val_acc,train_acc)\n",
        "#!cp 'new_CNN_hidden_size_800.pkl' 'drive/My Drive/Colab Notebooks/new_CNN_hidden_size_800.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lA-ixdtwp9Kg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "g = open(\"cnn_mul_hidden300_acc.pkl\",\"wb\")\n",
        "pickle.dump(cnn_mul_hidden_trainacc,g)\n",
        "g.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "05Q1ITVrqHGo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('cnn_mul_hidden300_acc.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5ePLJ_eeOO_g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# VAL ACC for CNN with hidden_size 300 and kernel_size 3:\n",
        "#cnn_mul_record = {}\n",
        "hidden_size = 300\n",
        "cnn_mul_300 = CNN_mul(hidden_size = hidden_size ,kernel_size = 3, padding_size = 1, num_layers = 2, num_classes = 3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  cnn_mul_300.cuda()\n",
        "train_loss, val_acc = train_model(train_loader,cnn_mul_300)\n",
        "torch.save(cnn_mul_300.state_dict(), 'CNN_mul_hidden_size_{}.pkl'.format(hidden_size))\n",
        "cnn_mul_record['hidden_size_{}'.format(hidden_size)] = (train_loss, val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dMzJ9QneOabq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_model(val_loader, cnn_mul_300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TEZYeMPab9SQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp 'CNN_mul_hidden_size_200.pkl' 'drive/My Drive/Colab Notebooks'\n",
        "!cp 'CNN_mul_hidden_size_300.pkl' 'drive/My Drive/Colab Notebooks'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T06MGi4eOgIj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# VAL ACC for CNN with hidden_size 400 and kernel_size 3:\n",
        "#cnn_mul_record = {}\n",
        "hidden_size = 400\n",
        "cnn_mul_400 = CNN_mul(hidden_size = hidden_size ,kernel_size = 3, padding_size = 1, num_layers = 2, num_classes = 3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  cnn_mul_400.cuda()\n",
        "train_loss, val_acc = train_model(train_loader,cnn_mul_400)\n",
        "torch.save(cnn_mul_400.state_dict(), 'CNN_mul_hidden_size_{}.pkl'.format(hidden_size))\n",
        "cnn_mul_record['hidden_size_{}'.format(hidden_size)] = (train_loss, val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Zp7NrB7OpbX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_model(val_loader, cnn_mul_400)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6krvJ4_xOyJD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_model(val_loader, cnn_mul_800)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O_NSAoGeNw7c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "g = open(\"cnn_mul_record.pkl\",\"wb\")\n",
        "pkl.dump(cnn_mul_record,g)\n",
        "g.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AXE5UxYiN-WR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('cnn_mul_record.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "srtNWOLsOCNv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp 'CNN_mul_hidden_size_200.pkl' 'drive/My Drive/Colab Notebooks'\n",
        "!cp 'CNN_mul_hidden_size_300.pkl' 'drive/My Drive/Colab Notebooks'\n",
        "!cp 'CNN_mul_hidden_size_400.pkl' 'drive/My Drive/Colab Notebooks'\n",
        "#!cp 'CNN_mul_hidden_size_800.pkl' 'drive/My Drive/Colab Notebooks'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4W88Ttjlw6P4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 4. Pick the best model and find 3 correct and 3 incorrect examples"
      ]
    },
    {
      "metadata": {
        "id": "BIJ7Y2kGxOEC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rnn_mul_800 = RNN_mul(hidden_size = 800, num_layers = 1, num_classes = 3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  rnn_mul_800.cuda()\n",
        "rnn_mul_800.load_state_dict(torch.load('drive/My Drive/Colab Notebooks/RNN_mul_hidden_size_800.pkl'))\n",
        "test_model(val_loader, rnn_mul_800)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eh_7uCcixNpN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_model(loader, model):\n",
        "    \"\"\"\n",
        "    Help function that tests the model's performance on a dataset\n",
        "    @param: loader - data loader for the dataset to test against\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    for data1, lengths1,data2, lengths2, labels in loader:\n",
        "        data1_batch, lengths1_batch,data2_batch, lengths2_batch, label_batch = data1, lengths1, data2, lengths2,labels\n",
        "        outputs =model(data1_batch, lengths1_batch,data2_batch, lengths2_batch)\n",
        "        predicted = outputs.max(1, keepdim=True)[1]\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
        "    return (100 * correct / total)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MBgXR8kZy9_B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#val_dataset_best = NewsGroupDataset(val_data_indices[0:200], y_val[0:200])\n",
        "val_loader_best = torch.utils.data.DataLoader(dataset=val_dataset, \n",
        "                                           batch_size=200,\n",
        "                                           collate_fn=newsgroup_collate_func,\n",
        "                                           shuffle=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9QT3kpdk1e6A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rnn_mul_800.eval()\n",
        "for data1, lengths1,data2, lengths2, labels in val_loader_best:\n",
        "  if use_cuda and torch.cuda.is_available():\n",
        "    data1 = data1.cuda()\n",
        "    lengths1  = lengths1.cuda()\n",
        "    data2 = data2.cuda()\n",
        "    lengths2  = lengths2.cuda()\n",
        "    labels = labels.cuda()\n",
        "  data1_batch, lengths1_batch,data2_batch, lengths2_batch, label_batch = data1, lengths1, data2, lengths2,labels\n",
        "  outputs = rnn_mul_800(data1_batch, lengths1_batch,data2_batch, lengths2_batch)\n",
        "  predicted = outputs.max(1, keepdim=True)[1]\n",
        "  \n",
        "  wrong_idx = [i[0] for i in (predicted.eq(labels.view_as(predicted))==0).nonzero().cpu().numpy()]\n",
        "  wrong_predict = [predicted.flatten().cpu().numpy()[i] for i in wrong_idx]\n",
        "  true_label = [label_batch.flatten().cpu().numpy()[i] for i in wrong_idx]\n",
        "  break\n",
        "\n",
        "print ('Wrong index :', wrong_idx)\n",
        "print('Wrong predict: ', wrong_predict)\n",
        "print('True label: ', true_label)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iaOvwUUv3aIZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(wrong_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8qVR8SSc7q4h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Incorrect 1"
      ]
    },
    {
      "metadata": {
        "id": "nrkkIuvC6MMS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Sent1:\\n ', ' '.join([id2token[i] for i in val_data[1][0] if i!=0]))\n",
        "print('Sent2:\\n ', ' '.join([id2token[i] for i in val_data[1][1] if i!=0]))\n",
        "print('True Label:',  val_targets[1])\n",
        "print('Wrong Predict:', wrong_predict[wrong_idx.index(1)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jrvz4BuD7x3M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Incorrect 2"
      ]
    },
    {
      "metadata": {
        "id": "24aF_iww7oXS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Sent1:\\n ', ' '.join([id2token[i] for i in val_data[23][0] if i!=0]))\n",
        "print('Sent2:\\n ', ' '.join([id2token[i] for i in val_data[23][1] if i!=0]))\n",
        "print('True Label:',  val_targets[23])\n",
        "print('Wrong Predict:', wrong_predict[wrong_idx.index(23)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F2FzT3jQ8gQg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Incorrect 3"
      ]
    },
    {
      "metadata": {
        "id": "zd71mG8T8j_x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Sent1:\\n ', ' '.join([id2token[i] for i in val_data[36][0] if i!=0]))\n",
        "print('Sent2:\\n ', ' '.join([id2token[i] for i in val_data[36][1] if i!=0]))\n",
        "print('True Label:',  val_targets[36])\n",
        "print('Wrong Predict:', wrong_predict[wrong_idx.index(36)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ni5GPois87CS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Correct 1"
      ]
    },
    {
      "metadata": {
        "id": "EBi1Mcws85X5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Sent1:\\n ', ' '.join([id2token[i] for i in val_data[2][0] if i!=0]))\n",
        "print('Sent2:\\n ', ' '.join([id2token[i] for i in val_data[2][1] if i!=0]))\n",
        "print('True Label:',  val_targets[2])\n",
        "print('Predict:', predicted.flatten().cpu().numpy()[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4h9Ck47d9rmr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Correct 2"
      ]
    },
    {
      "metadata": {
        "id": "8E-tfW8A9xWu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Sent1:\\n ', ' '.join([id2token[i] for i in val_data[27][0] if i!=0]))\n",
        "print('Sent2:\\n ', ' '.join([id2token[i] for i in val_data[27][1] if i!=0]))\n",
        "print('True Label:',  val_targets[27])\n",
        "print('Predict:', predicted.flatten().cpu().numpy()[27])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LpHUPti-9sgg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Correct 3"
      ]
    },
    {
      "metadata": {
        "id": "Onc3tr_Z9ppv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Sent1:\\n ', ' '.join([id2token[i] for i in val_data[199][0] if i!=0]))\n",
        "print('Sent2:\\n ', ' '.join([id2token[i] for i in val_data[199][1] if i!=0]))\n",
        "print('True Label:',  val_targets[199])\n",
        "print('Predict:', predicted.flatten().cpu().numpy()[199])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c4fuZeudGTvD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Evaluation on _mnli val_"
      ]
    },
    {
      "metadata": {
        "id": "QN1QcXTBGvuq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv('mnli_val.tsv', sep=\"\\t\", index_col=False )\n",
        "test_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Id5elBjOIWUj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_df['genre'].unique()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6zs7wk1nG9k7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def df2idx_mnli(fname,genre):\n",
        "    df = pd.read_csv(fname, sep=\"\\t\", index_col=False )\n",
        "    df = df[df.genre == genre]\n",
        "    # change the label to numerical value\n",
        "    df.loc[df['label'] == 'entailment', 'label'] = 0\n",
        "    df.loc[df['label'] == 'contradiction', 'label'] = 1\n",
        "    df.loc[df['label'] == 'neutral', 'label'] = 2\n",
        "    # convert token to idx\n",
        "    df['sent1_idx']  = df.apply (lambda row:[token2id[token] if token in token2id else UNK_IDX for token in row.sentence1.split()],axis=1)\n",
        "    df['sent2_idx']  = df.apply (lambda row:[token2id[token] if token in token2id else UNK_IDX for token in row.sentence2.split()],axis=1)\n",
        "    # convert df to data list and label list\n",
        "    indexed_data = list(zip(df.sent1_idx, df.sent2_idx))\n",
        "    label = np.array(df.label)\n",
        "    return indexed_data, label, df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w2ujNvIjH0Ss",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fic_test_data, fic_test_targets,_= df2idx_mnli(\"mnli_val.tsv\", 'fiction')\n",
        "tel_test_data, tel_test_targets,_= df2idx_mnli(\"mnli_val.tsv\", 'telephone')\n",
        "sla_test_data, sla_test_targets,_= df2idx_mnli(\"mnli_val.tsv\", 'slate')\n",
        "gov_test_data, gov_test_targets,_= df2idx_mnli(\"mnli_val.tsv\", 'government')\n",
        "tra_test_data, tra_test_targets,_= df2idx_mnli(\"mnli_val.tsv\", 'travel')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2ajHNOimrFkL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fic_train_data, fic_train_targets,_= df2idx_mnli(\"mnli_train.tsv\", 'fiction')\n",
        "tel_train_data, tel_train_targets,_= df2idx_mnli(\"mnli_train.tsv\", 'telephone')\n",
        "sla_train_data, sla_train_targets,_= df2idx_mnli(\"mnli_train.tsv\", 'slate')\n",
        "gov_train_data, gov_train_targets,_= df2idx_mnli(\"mnli_train.tsv\", 'government')\n",
        "tra_train_data, tra_train_targets,_= df2idx_mnli(\"mnli_train.tsv\", 'travel')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aQCzF4EEJI10",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fic_test_dataset =  NewsGroupDataset(fic_test_data, fic_test_targets)\n",
        "fic_test_loader = torch.utils.data.DataLoader(dataset=fic_test_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=newsgroup_collate_func,\n",
        "                                           shuffle=False)\n",
        "tel_test_dataset =  NewsGroupDataset(tel_test_data, tel_test_targets)\n",
        "tel_test_loader = torch.utils.data.DataLoader(dataset=tel_test_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=newsgroup_collate_func,\n",
        "                                           shuffle=False)\n",
        "sla_test_dataset =  NewsGroupDataset(sla_test_data, sla_test_targets)\n",
        "sla_test_loader = torch.utils.data.DataLoader(dataset=sla_test_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=newsgroup_collate_func,\n",
        "                                           shuffle=False)\n",
        "gov_test_dataset =  NewsGroupDataset(gov_test_data, gov_test_targets)\n",
        "gov_test_loader = torch.utils.data.DataLoader(dataset=gov_test_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=newsgroup_collate_func,\n",
        "                                           shuffle=False)\n",
        "tra_test_dataset =  NewsGroupDataset(tra_test_data, tra_test_targets)\n",
        "tra_test_loader = torch.utils.data.DataLoader(dataset=tra_test_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=newsgroup_collate_func,\n",
        "                                           shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_cDMsJO2rclt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fic_train_dataset =  NewsGroupDataset(fic_train_data, fic_train_targets)\n",
        "fic_train_loader = torch.utils.data.DataLoader(dataset=fic_train_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=newsgroup_collate_func,\n",
        "                                           shuffle=False)\n",
        "tel_train_dataset =  NewsGroupDataset(tel_train_data, tel_train_targets)\n",
        "tel_train_loader = torch.utils.data.DataLoader(dataset=tel_train_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=newsgroup_collate_func,\n",
        "                                           shuffle=False)\n",
        "sla_train_dataset =  NewsGroupDataset(sla_train_data, sla_train_targets)\n",
        "sla_train_loader = torch.utils.data.DataLoader(dataset=sla_train_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=newsgroup_collate_func,\n",
        "                                           shuffle=False)\n",
        "gov_train_dataset =  NewsGroupDataset(gov_train_data, gov_train_targets)\n",
        "gov_train_loader = torch.utils.data.DataLoader(dataset=gov_train_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=newsgroup_collate_func,\n",
        "                                           shuffle=False)\n",
        "tra_train_dataset =  NewsGroupDataset(tra_train_data, tra_train_targets)\n",
        "tra_train_loader = torch.utils.data.DataLoader(dataset=tra_train_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=newsgroup_collate_func,\n",
        "                                           shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e8XU5DlWPK0u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5.1 Evaluation of RNN"
      ]
    },
    {
      "metadata": {
        "id": "1WaRfRcGSHx_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 5.1.1 Load Best RNN Model\n",
        "- hidden size :800\n",
        "- concatenate two encoded sentences with element-wise multiplication"
      ]
    },
    {
      "metadata": {
        "id": "I5SXtWcQScqz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rnn_mul_800 = RNN_mul(hidden_size = 800, num_layers = 1, num_classes = 3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  rnn_mul_800.cuda()\n",
        "rnn_mul_800.load_state_dict(torch.load('drive/My Drive/Colab Notebooks/RNN_mul_hidden_size_800.pkl'))\n",
        "#test_model(val_loader, rnn_mul_800)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s_AYb7zRPxFJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 5.1.2 Evaluate across genres and generate val acc table "
      ]
    },
    {
      "metadata": {
        "id": "bFh4WAEESmPr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rnn_val_acc_genre = []\n",
        "for data_loader in [fic_test_loader, tel_test_loader, sla_test_loader, gov_test_loader, tra_test_loader]:\n",
        "  rnn_val_acc_genre.append(test_model(data_loader, rnn_mul_800))\n",
        "genres = ['fiction', 'telephone', 'slate', 'government', 'travel']\n",
        "rnn_mnli_acc = pd.DataFrame(data = {'Genre': genres, 'Validation Accuracy':rnn_val_acc_genre})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4ZMPmLTk-tEM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rnn_mnli_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UYP0qoBD-yIG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rnn_mnli_acc.to_csv('MNLI_rnn_acc.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gXmqqOz9PU9C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5.2 Evaluation of CNN"
      ]
    },
    {
      "metadata": {
        "id": "yeXNvxWaT_P1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###5.2.1 Load Best CNN Model \n",
        "- hidden size: \n",
        "- concatenate two encoded sentences with element-wise multiplication"
      ]
    },
    {
      "metadata": {
        "id": "NlbVg_BgPZ1d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cnn_300 = CNN(hidden_size = 300, kernel_size = 3, padding_size = 1,num_layers = 2, num_classes = 3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  cnn_300.cuda()\n",
        "cnn_300.load_state_dict(torch.load('drive/My Drive/Colab Notebooks/CNN_hidden_size_300.pkl'))\n",
        "#test_model(val_loader, cnn_300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lh9ZJkwmUdkv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 5.2.2 Evaluate across genres and generate val acc table"
      ]
    },
    {
      "metadata": {
        "id": "9GLsqDaZUi7T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cnn_val_acc_genre = []\n",
        "for data_loader in [fic_test_loader, tel_test_loader, sla_test_loader, gov_test_loader, tra_test_loader]:\n",
        "  cnn_val_acc_genre.append(test_model(data_loader, cnn_300))\n",
        "genres = ['fiction', 'telephone', 'slate', 'government', 'travel']\n",
        "cnn_mnli_acc = pd.DataFrame(data = {'Genre': genres, 'Validation Accuracy':cnn_val_acc_genre})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gLL6LgcxHAzO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cnn_mnli_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YqjQ9mbIHKAt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cnn_mnli_acc.to_csv('MNLI_cnn_acc.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_7aqP4TkoheN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yXV1PlOHq1AK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning MultiNLI"
      ]
    },
    {
      "metadata": {
        "id": "jt3dH6VssLUs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fine_tuning(train_loader,val_loader, model):\n",
        "  criterion = torch.nn.NLLLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  total_step = len(train_loader)\n",
        "  train_loss_ls = []\n",
        "  val_acc_ls = []\n",
        "  for epoch in range(num_epochs):\n",
        "      loss_batch = []\n",
        "      for i, (data1, lengths1, data2, lengths2, labels) in enumerate(train_loader):\n",
        "  #         data1 = Variable(data1)  \n",
        "  #         lengths1 = Variable(lengths1)\n",
        "  #         data2 = Variable(data2)  \n",
        "  #         lengths2 = Variable(lengths2)# Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
        "  #         labels = Variable(labels)\n",
        "          if use_cuda and torch.cuda.is_available():\n",
        "              data1 = data1.cuda()\n",
        "              lengths1  = lengths1.cuda()\n",
        "              data2 = data2.cuda()\n",
        "              lengths2  = lengths2.cuda()\n",
        "              labels = labels.cuda()\n",
        "          model.train()\n",
        "          optimizer.zero_grad()\n",
        "          # Forward pass\n",
        "          outputs = model(data1, lengths1, data2, lengths2)\n",
        "          predicted = outputs.max(1, keepdim=True)[1]\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss_batch.append(loss.item())\n",
        "          # Backward and optimize\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          # validate every 100 iterations\n",
        "          if i > 0 and i % 50 == 0:\n",
        "              # validate\n",
        "              train_loss = loss_batch[i]\n",
        "              val_acc = test_model(val_loader, model)\n",
        "              train_loss_ls.append(train_loss)\n",
        "              val_acc_ls.append(val_acc)\n",
        "              print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}, Training Loss: {}'.format(\n",
        "                         epoch+1, num_epochs, i+1, len(train_loader), val_acc,train_loss))\n",
        "  #torch.save(model_object.state_dict(), 'params_{}.pkl'.format())\n",
        "#model_object.load_state_dict(torch.load('params.pkl'))\n",
        "  return train_loss_ls, val_acc_ls\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JeujrhIrsmvV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "fine_tuning_dic = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pQSRO444q9dM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rnn_fic = RNN_mul(hidden_size = 800, num_layers = 1, num_classes = 3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  rnn_fic.cuda()\n",
        "rnn_fic.load_state_dict(torch.load('drive/My Drive/Colab Notebooks/RNN_mul_hidden_size_800.pkl'))\n",
        "fine_tuning_dic['fic'] = fine_tuning(fic_train_loader,fic_test_loader,rnn_fic )\n",
        "torch.save(rnn_fic.state_dict(), 'RNN_finetuning_FIC.pkl')\n",
        "#cnn_mul_record['hidden_size_{}'.format(hidden_size)] = (train_loss, val_acc)\n",
        "#!cp 'RNN_finetuning_FIC.pkl' 'drive/My Drive/Colab Notebooks/RNN_finetuning_FIC.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BZ87mPIPrpFk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_model(fic_test_loader,rnn_fic)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WixNI0CSqsxz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "rnn_tel = RNN_mul(hidden_size = 800, num_layers = 1, num_classes = 3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  rnn_tel.cuda()\n",
        "rnn_tel.load_state_dict(torch.load('drive/My Drive/Colab Notebooks/RNN_mul_hidden_size_800.pkl'))\n",
        "fine_tuning_dic['tel'] = fine_tuning(tel_train_loader,tel_test_loader,rnn_tel )\n",
        "torch.save(rnn_tel.state_dict(), 'RNN_finetuning_TEL.pkl')\n",
        "#cnn_mul_record['hidden_size_{}'.format(hidden_size)] = (train_loss, val_acc)\n",
        "!cp 'RNN_finetuning_TEL.pkl' 'drive/My Drive/Colab Notebooks/RNN_finetuning_TEL.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cRyLdAGkrwj6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_model(tel_test_loader,rnn_tel)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5zcfqv4JrRnB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "rnn_sla= RNN_mul(hidden_size = 800, num_layers = 1, num_classes = 3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  rnn_sla.cuda()\n",
        "rnn_sla.load_state_dict(torch.load('drive/My Drive/Colab Notebooks/RNN_mul_hidden_size_800.pkl'))\n",
        "fine_tuning_dic['sla'] = fine_tuning(sla_train_loader,sla_test_loader,rnn_sla )\n",
        "torch.save(rnn_sla.state_dict(), 'RNN_finetuning_SLA.pkl')\n",
        "#cnn_mul_record['hidden_size_{}'.format(hidden_size)] = (train_loss, val_acc)\n",
        "!cp 'RNN_finetuning_SLA.pkl' 'drive/My Drive/Colab Notebooks/RNN_finetuning_SLA.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s5EpIl0FsieS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_model(sla_test_loader,rnn_sla)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8KX7K6-ZsPsj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "rnn_gov= RNN_mul(hidden_size = 800, num_layers = 1, num_classes = 3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  rnn_gov.cuda()\n",
        "rnn_gov.load_state_dict(torch.load('drive/My Drive/Colab Notebooks/RNN_mul_hidden_size_800.pkl'))\n",
        "fine_tuning_dic['gov'] = fine_tuning(gov_train_loader,gov_test_loader,rnn_gov )\n",
        "torch.save(rnn_gov.state_dict(), 'RNN_finetuning_GOV.pkl')\n",
        "#cnn_mul_record['hidden_size_{}'.format(hidden_size)] = (train_loss, val_acc)\n",
        "!cp 'RNN_finetuning_GOV.pkl' 'drive/My Drive/Colab Notebooks/RNN_finetuning_GOV.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LuFZzc3ksmk0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_model(gov_test_loader,rnn_gov)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2wd3ioE5tjyh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "rnn_tra= RNN_mul(hidden_size = 800, num_layers = 1, num_classes = 3, pre_trained_emb = ft_emb)\n",
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  rnn_tra.cuda()\n",
        "rnn_tra.load_state_dict(torch.load('drive/My Drive/Colab Notebooks/RNN_mul_hidden_size_800.pkl'))\n",
        "fine_tuning_dic['tra'] = fine_tuning(tra_train_loader,tra_test_loader,rnn_gov )\n",
        "torch.save(rnn_tra.state_dict(), 'RNN_finetuning_TRA.pkl')\n",
        "#cnn_mul_record['hidden_size_{}'.format(hidden_size)] = (train_loss, val_acc)\n",
        "!cp 'RNN_finetuning_TRA.pkl' 'drive/My Drive/Colab Notebooks/RNN_finetuning_TRA.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BIX3z_WjtvPn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_model(tra_test_loader,rnn_tra)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6hJX63Iqvhq-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}